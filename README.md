# ACRONIM: An explanation system for comparing activation vs. importance of multimodal concepts

This repository is a fork of https://github.com/mshukor/xl-vlms. It extends their work in https://arxiv.org/html/2406.08074v3 to include concept importance estimation using Gradient * Concept (an adaptation of Gradient * Input to concept activation vectors learned by CoX-LMM). We provide a frontend for programmatically generating and comparing most activating/most important concept vectors; visualizing their visual and textual groundings; and utilities for computing concept attribution metrics such as C-Deletion.

This was created as a final course project for the **ECS289H - Explainable AI using Visualization** course at UC Davis, Fall 2025.


## Setup

### XL-VLM library setup
On the machine where you're hosting the backend:
1. Clone repo
2. Download MS COCO dataset
3. Download Karpathy splits
4. Update paths: `DATA_DIR, ANNOTATION_FILE, OUT_DIR` under `src/importance_estimation.py`
5. Install miniconda
6. Install Packages
    ```bash
    conda create --name xl_vlm python=3.9
    conda activate xl_vlm
    pip install -e .
    conda install -c bioconda perl-xml-libxml
    conda install -c conda-forge openjdk
    pip install git+https://github.com/bckim92/language-evaluation.git
    python -c "import language_evaluation; language_evaluation.download('coco')"
    import nltk
    nltk.download('words')
    ```
7. Set `HF_HOME` environment variable to whatever directory you'd like to store model caches in.

### Backend server setup (FastAPI + Uvicorn)
On the same machine where you set up the XL-VLM library (as above):
1. `conda activate xl_vlm`
2. `pip install fastapi uvicorn`
3. `uvicorn main:app --host 0.0.0.0 --port 8000 --reload`

4. If you are hosting the backend on a separate server with GPU compute, you may need to run this in a terminal on the machine where you're hosting the frontend:

    `ssh -fNT -L 8000:localhost:8000 username@server.address`

    This starts the `ssh` tunnel in the background. To close the tunnel, find the ssh PID with:

        ps aux | head -1; ps aux | grep "ssh -L"
    
    then

        kill <PID>

On the frontend:

5. Visit `http://localhost:8000/docs`. You should see the autogenerated SwaggerUI doc page.

### Frontend setup (Vite + React)
(WIP)

## Usage

### Frontend dashboard
(WIP)

### Manual run of concept importance estimation
(WIP)

    conda activate xl_vlm
    python ./src/importance_estimation.py

### Metrics
(WIP)